# ğŸŒŒ Desafio de Data Lake: Filmes e SÃ©ries

## ğŸš€ Transformando Dados em Conhecimento â€“ Uma Jornada com Filmes e SÃ©ries!
1. Objetivo
2. Iniciando a MissÃ£o
3. Entrega 1 - IngestÃ£o Batch
4. EvidÃªncias & Screenshots
5. ConclusÃ£o

### ğŸ¬Ao longo desse desafio, vamos explorar ingestÃ£o, armazenamento, processamento e anÃ¡lise de dados. Ao final, seremos capazes de responder perguntas como:


ğŸŒŸ Nosso foco nesta entrega:
Realizar a ingestÃ£o batch de arquivos locais para a AWS S3 em uma estrutura organizada, automatizando o processo com Docker e boto3.


-- Iniciando a MissÃ£o
PrÃ©-requisitos:
Baixe o arquivo de dados: Filmes e Series.zip.
Uma conta AWS para manipular o S3.
Desenvolvimento com o uso de Docker e Python.

### ğŸ¬ Explorar o arquivo CSV para definir o tipo de analise...

-- Entrega 1 - IngestÃ£o Batch

Vamos criar um pipeline de ingestÃ£o que carrega dados CSV locais para o AWS S3, onde serÃ£o organizados com uma estrutura de armazenamento especÃ­fica.

Estrutura no S3:
Os dados devem ser organizados no bucket conforme o formato:

Modo de Estrutura:

Filmes: S3://data-lake-desafio/RAW/LocalCSV/Movies/YYYYMMDD/movies.csv
SÃ©ries: S3://data-lake-desafio/RAW/LocalCSV/Series/YYYYMMDD/series.csv

### ğŸš§ Passos de ImplementaÃ§Ã£o:
1. ImplementaÃ§Ã£o do CÃ³digo Python
Arquivo: awsbucket.py

Este cÃ³digo lÃª e carrega arquivos CSV no S3, utilizando a biblioteca boto3. Arquivos contendo â€œmoviesâ€ ou â€œseriesâ€ no nome sÃ£o direcionados para pastas especÃ­ficas.

## ğŸ“„ awsbucket.py

## [codigo-python](../desafio/awsbucket.py)


## 2. ConfiguraÃ§Ã£o do Docker
Arquivo: Dockerfile

--Este Dockerfile cria um ambiente para rodar nosso script de upload.

## ğŸ“„ Dockerfile
### [DOCKERFILE](../desafio/dockerfile)


3. ConstruÃ§Ã£o e ExecuÃ§Ã£o do Docker
Construindo a imagem Docker e o container..

## Construir a imagem
docker build -t sprint6 .

# Executar o container criando volume
docker run -v C:/Users/Otavi/Downloads/Filmes+e+Series:/app/data sprint6:
![imagem-docker](../evidencias/docker_desafio.png)


 # EvidÃªncias & Screenshots

#### Aqui estÃ£o alguns registros da execuÃ§Ã£o do desafio:
### [evidencias](/SPRINT6/evidencias/)

 ConclusÃ£o
Essa primeira etapa do desafio foi um exercÃ­cio de criaÃ§Ã£o de pipelines de dados e uso do Docker para automatizaÃ§Ã£o de processos. Neste ponto, a estrutura inicial do Data Lake foi configurada, e os dados foram carregados no S3. Na prÃ³xima etapa, exploraremos o processamento e a organizaÃ§Ã£o desses dados para anÃ¡lise!

ğŸ“Š 
